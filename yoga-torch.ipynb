{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10162104,"sourceType":"datasetVersion","datasetId":6275166}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"train_dir = os.path.join(data_dir, 'train')\nval_dir = os.path.join(data_dir, 'val')\n\nas i mentioned... that.. \nthere is no directory.. of test and train...there is only text file in which there are contents.. like..>\n\"Akarna_Dhanurasana/16.jpg,1,8,0\nAkarna_Dhanurasana/95.jpg,1,8,0\nAkarna_Dhanurasana/263.jpg,1,8,0\nAkarna_Dhanurasana/110.jpg,1,8,0\nAkarna_Dhanurasana/21.jpg,1,8,0\nAkarna_Dhanurasana/55.jpg,1,8,0\nAkarna_Dhanurasana/34.jpg,1,8,0\"\n\nand the images.. are in..\n/kaggle/input/82yogaclasses/yoga_images/images...\n\n\nand the text files...\n/kaggle/input/82yogaclasses/yoga_test.txt\n/kaggle/input/82yogaclasses/yoga_train.txt\n\n\nhere is one more thing from where i donloaded dataset..\n\"This repository contains: \r\n1> Train and test split\r\n\tyoga_train.txt\r\n\tyoga_test.txt\r\n\r\nEach of the split text file structure\r\nrow-> each row corresponds to each sample detail\r\ncol-> <image_address+name> , <label of class_6> , <label of class_20> , <label of class_82>\r\n\r\n2> 82 text files correspond to each of the yoga pose with  \r\nrow-> each row corresponds to each sample detail\r\ncol-> <image_address+name> , <url>\r\n\r\nSave the images in same <folder/image> name as given in 82 text files.\r\n\r\n\r\nPlease cite the following paper if you use this dataset.\r\n\r\n@article{verma2020yoga,\r\n  title={Yoga-82: A New Dataset for Fine-grained Classification of Human Poses},\r\n  author={Verma, Manisha and Kumawat, Sudhakar and Nakashima, Yuta and Raman, Shanmuganathan},\r\n  journal={arXiv preprint arXiv:2004.10362},\r\n  year={2020}\r\n}\"","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:33:12.667765Z","iopub.execute_input":"2024-12-11T13:33:12.668174Z","iopub.status.idle":"2024-12-11T13:33:12.674215Z","shell.execute_reply.started":"2024-12-11T13:33:12.668136Z","shell.execute_reply":"2024-12-11T13:33:12.673048Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:33:17.102435Z","iopub.execute_input":"2024-12-11T13:33:17.103054Z","iopub.status.idle":"2024-12-11T13:33:17.109348Z","shell.execute_reply.started":"2024-12-11T13:33:17.102996Z","shell.execute_reply":"2024-12-11T13:33:17.107899Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Paths\ndata_dir = \"/kaggle/input/82yogaclasses/yoga_images\"  # Replace with your dataset path\nyoga_train_file = \"/kaggle/input/82yogaclasses/yoga_train.txt\"  # Replace with actual train.txt\nyoga_test_file = \"/kaggle/input/82yogaclasses/yoga_test.txt\"  # Replace with actual test.txt\nnot_found_file = \"/kaggle/working/not_found_images.txt\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:33:25.656035Z","iopub.execute_input":"2024-12-11T13:33:25.656411Z","iopub.status.idle":"2024-12-11T13:33:25.662994Z","shell.execute_reply.started":"2024-12-11T13:33:25.656381Z","shell.execute_reply":"2024-12-11T13:33:25.661637Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"validate_images(yoga_train_file, os.path.join(data_dir, 'train'))\nvalidate_images(yoga_test_file, os.path.join(data_dir, 'test'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:33:47.435778Z","iopub.execute_input":"2024-12-11T13:33:47.436138Z","iopub.status.idle":"2024-12-11T13:33:47.634152Z","shell.execute_reply.started":"2024-12-11T13:33:47.436108Z","shell.execute_reply":"2024-12-11T13:33:47.632984Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Initialize class names from train.txt\ndef extract_classes(file_path):\n    classes = set()\n    with open(file_path, 'r') as f:\n        for line in f:\n            image_path = line.strip()\n            label = os.path.basename(os.path.dirname(image_path))\n            classes.add(label)\n    return sorted(list(classes))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:33:49.646165Z","iopub.execute_input":"2024-12-11T13:33:49.646563Z","iopub.status.idle":"2024-12-11T13:33:49.653426Z","shell.execute_reply.started":"2024-12-11T13:33:49.646527Z","shell.execute_reply":"2024-12-11T13:33:49.652221Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Extract class names\nclass_names = extract_classes(yoga_train_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:33:50.867728Z","iopub.execute_input":"2024-12-11T13:33:50.868123Z","iopub.status.idle":"2024-12-11T13:33:50.915850Z","shell.execute_reply.started":"2024-12-11T13:33:50.868089Z","shell.execute_reply":"2024-12-11T13:33:50.914512Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Validate images function\nmissing_images = []\nfound_count = 0\nmissing_count = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:33:57.764181Z","iopub.execute_input":"2024-12-11T13:33:57.764538Z","iopub.status.idle":"2024-12-11T13:33:57.770945Z","shell.execute_reply.started":"2024-12-11T13:33:57.764506Z","shell.execute_reply":"2024-12-11T13:33:57.769709Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"\ndef validate_images(file_path):\n    global found_count, missing_count\n    with open(file_path, 'r') as f:\n        for line in f:\n            image_path = line.strip()\n            if not os.path.exists(image_path):\n                missing_images.append(image_path)\n                missing_count += 1\n            else:\n                found_count += 1\n\n    # Save missing images to a file\n    if missing_images:\n        with open(not_found_file, 'w') as nf:\n            nf.write(\"\\n\".join(missing_images))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:34:02.340108Z","iopub.execute_input":"2024-12-11T13:34:02.340503Z","iopub.status.idle":"2024-12-11T13:34:02.347973Z","shell.execute_reply.started":"2024-12-11T13:34:02.340468Z","shell.execute_reply":"2024-12-11T13:34:02.346573Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def generate_file_list(data_dir, split_name, output_file):\n    split_dir = os.path.join(data_dir, split_name)\n    with open(output_file, \"w\") as f:\n        for root, _, files in os.walk(split_dir):\n            for file in files:\n                f.write(os.path.join(root, file) + \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:34:06.303535Z","iopub.execute_input":"2024-12-11T13:34:06.303965Z","iopub.status.idle":"2024-12-11T13:34:06.310921Z","shell.execute_reply.started":"2024-12-11T13:34:06.303931Z","shell.execute_reply":"2024-12-11T13:34:06.309437Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"generate_file_list(data_dir, \"train\", \"/kaggle/input/82yogaclasses/yoga_train.txt\")\ngenerate_file_list(data_dir, \"test\", \"/kaggle/input/82yogaclasses/yoga_test.txt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:34:09.084190Z","iopub.execute_input":"2024-12-11T13:34:09.084672Z","iopub.status.idle":"2024-12-11T13:34:09.138967Z","shell.execute_reply.started":"2024-12-11T13:34:09.084588Z","shell.execute_reply":"2024-12-11T13:34:09.137268Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_file_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/82yogaclasses/yoga_train.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m generate_file_list(data_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/82yogaclasses/yoga_test.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[33], line 3\u001b[0m, in \u001b[0;36mgenerate_file_list\u001b[0;34m(data_dir, split_name, output_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_file_list\u001b[39m(data_dir, split_name, output_file):\n\u001b[1;32m      2\u001b[0m     split_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, split_name)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m root, _, files \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(split_dir):\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/kaggle/input/82yogaclasses/yoga_train.txt'"],"ename":"OSError","evalue":"[Errno 30] Read-only file system: '/kaggle/input/82yogaclasses/yoga_train.txt'","output_type":"error"}],"execution_count":34},{"cell_type":"code","source":"# Validate train and test images\nprint(\"Validating train images...\")\nvalidate_images(yoga_train_file)\nprint(\"Validating test images...\")\nvalidate_images(yoga_test_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:34:16.471334Z","iopub.execute_input":"2024-12-11T13:34:16.471833Z","iopub.status.idle":"2024-12-11T13:34:16.583480Z","shell.execute_reply.started":"2024-12-11T13:34:16.471795Z","shell.execute_reply":"2024-12-11T13:34:16.582134Z"}},"outputs":[{"name":"stdout","text":"Validating train images...\nValidating test images...\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"with open(\"train.txt\", \"w\") as train_file:\n    for root, _, files in os.walk(os.path.join(data_dir, \"train\")):\n        for file in files:\n            train_file.write(os.path.relpath(os.path.join(root, file), data_dir) + \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:34:24.224332Z","iopub.execute_input":"2024-12-11T13:34:24.224966Z","iopub.status.idle":"2024-12-11T13:34:24.233894Z","shell.execute_reply.started":"2024-12-11T13:34:24.224902Z","shell.execute_reply":"2024-12-11T13:34:24.231975Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# Display counts\nprint(f\"Found: {found_count}, Missing: {missing_count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:34:28.534077Z","iopub.execute_input":"2024-12-11T13:34:28.534465Z","iopub.status.idle":"2024-12-11T13:34:28.540984Z","shell.execute_reply.started":"2024-12-11T13:34:28.534429Z","shell.execute_reply":"2024-12-11T13:34:28.539857Z"}},"outputs":[{"name":"stdout","text":"Found: 0, Missing: 28450\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# Debugging\nif total_missing > 0:\n    print(\"Missing files summary:\")\n    print(\"\\n\".join(missing_images[:10]))  # Display first 10 missing paths for inspection\n    print(f\"...and {total_missing - 10} more.\") if total_missing > 10 else None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:47:14.531559Z","iopub.execute_input":"2024-12-11T12:47:14.532260Z","iopub.status.idle":"2024-12-11T12:47:14.537152Z","shell.execute_reply.started":"2024-12-11T12:47:14.532225Z","shell.execute_reply":"2024-12-11T12:47:14.536248Z"}},"outputs":[{"name":"stdout","text":"Missing files summary:\n/kaggle/input/82yogaclasses/yoga_images/Akarna_Dhanurasana/64.jpg,1,8,0\n/kaggle/input/82yogaclasses/yoga_images/Akarna_Dhanurasana/229.jpg,1,8,0\n/kaggle/input/82yogaclasses/yoga_images/Akarna_Dhanurasana/128.jpg,1,8,0\n/kaggle/input/82yogaclasses/yoga_images/Akarna_Dhanurasana/145.jpg,1,8,0\n/kaggle/input/82yogaclasses/yoga_images/Akarna_Dhanurasana/47.jpg,1,8,0\n/kaggle/input/82yogaclasses/yoga_images/Akarna_Dhanurasana/121.jpg,1,8,0\n/kaggle/input/82yogaclasses/yoga_images/Akarna_Dhanurasana/100.jpg,1,8,0\n/kaggle/input/82yogaclasses/yoga_images/Akarna_Dhanurasana/56.jpg,1,8,0\n/kaggle/input/82yogaclasses/yoga_images/Akarna_Dhanurasana/200.jpg,1,8,0\n/kaggle/input/82yogaclasses/yoga_images/Akarna_Dhanurasana/26.jpg,1,8,0\n...and 28440 more.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Hyperparameters\nIMG_SIZE = 224\nBATCH_SIZE = 32\nEPOCHS = 100\nLEARNING_RATE = 0.001\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:47:16.147459Z","iopub.execute_input":"2024-12-11T12:47:16.148165Z","iopub.status.idle":"2024-12-11T12:47:16.152129Z","shell.execute_reply.started":"2024-12-11T12:47:16.148133Z","shell.execute_reply":"2024-12-11T12:47:16.151133Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"\n# Classes (optional: replace with actual class names)\nclass_names = os.listdir(os.path.join(data_dir, 'train'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:47:16.837314Z","iopub.execute_input":"2024-12-11T12:47:16.838062Z","iopub.status.idle":"2024-12-11T12:47:16.859912Z","shell.execute_reply.started":"2024-12-11T12:47:16.838026Z","shell.execute_reply":"2024-12-11T12:47:16.858584Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Classes (optional: replace with actual class names)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m class_names \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/82yogaclasses/yoga_images/train'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/82yogaclasses/yoga_images/train'","output_type":"error"}],"execution_count":32},{"cell_type":"code","source":"# Transformations for data preprocessing and augmentation\ntransform_train = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:41:59.202793Z","iopub.execute_input":"2024-12-11T12:41:59.203692Z","iopub.status.idle":"2024-12-11T12:41:59.208830Z","shell.execute_reply.started":"2024-12-11T12:41:59.203656Z","shell.execute_reply":"2024-12-11T12:41:59.207816Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"transform_val = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:42:00.380151Z","iopub.execute_input":"2024-12-11T12:42:00.380820Z","iopub.status.idle":"2024-12-11T12:42:00.385297Z","shell.execute_reply.started":"2024-12-11T12:42:00.380772Z","shell.execute_reply":"2024-12-11T12:42:00.384349Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Load Dataset\ntrain_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=transform_train)\nval_dataset = datasets.ImageFolder(os.path.join(data_dir, 'val'), transform=transform_val)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:42:01.326031Z","iopub.execute_input":"2024-12-11T12:42:01.326375Z","iopub.status.idle":"2024-12-11T12:42:01.405607Z","shell.execute_reply.started":"2024-12-11T12:42:01.326344Z","shell.execute_reply":"2024-12-11T12:42:01.404292Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load Dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m), transform\u001b[38;5;241m=\u001b[39mtransform_val)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    321\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    327\u001b[0m ):\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[1;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/82yogaclasses/yoga_images/train'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/82yogaclasses/yoga_images/train'","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize Sample Images\ndef show_images(loader):\n    inputs, labels = next(iter(loader))\n    grid = torchvision.utils.make_grid(inputs[:16], nrow=4)\n    plt.figure(figsize=(10, 10))\n    plt.imshow(grid.permute(1, 2, 0))  # Convert to HWC\n    plt.axis(\"off\")\n    plt.show()\nprint(\"Sample Images:\")\nshow_images(train_loader)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Models to Test\nmodels_to_test = {\n    \"ResNet18\": models.resnet18(pretrained=True),\n    \"VGG16\": models.vgg16(pretrained=True),\n    \"EfficientNetB0\": models.efficientnet_b0(pretrained=True),\n    \"DenseNet121\": models.densenet121(pretrained=True),\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Modify final layers for classification\nfor name, model in models_to_test.items():\n    if \"fc\" in dir(model):  # For ResNet-like models\n        model.fc = nn.Sequential(\n            nn.Linear(model.fc.in_features, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, len(class_names))\n        )\n    elif \"classifier\" in dir(model):  # For VGG-like models\n        model.classifier[-1] = nn.Sequential(\n            nn.Linear(model.classifier[-1].in_features, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, len(class_names))\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training Function\ndef train_model(model, train_loader, val_loader, criterion, optimizer, epochs=EPOCHS):\n    best_val_acc = 0\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch + 1}/{epochs}\")\n        model.train()\n        train_loss = 0\n        correct = 0\n        total = 0\n\n        for inputs, labels in tqdm(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n        train_acc = correct / total\n        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n\n        # Validation\n        model.eval()\n        val_loss = 0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item()\n                _, preds = torch.max(outputs, 1)\n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n\n        val_acc = correct / total\n        print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n\n        # Save the best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), f\"{model.__class__.__name__}_best.pth\")\n            print(f\"Saved Best Model: {model.__class__.__name__}\")\n\n    return best_val_acc\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train and Evaluate Models\ncriterion = nn.CrossEntropyLoss()\nresults = {}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for model_name, model in models_to_test.items():\n    print(f\"\\nTraining {model_name}...\")\n    model = model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    acc = train_model(model, train_loader, val_loader, criterion, optimizer, epochs=EPOCHS)\n    results[model_name] = acc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display Results\nbest_model_name = max(results, key=results.get)\nprint(f\"Best Model: {best_model_name} with Accuracy: {results[best_model_name]:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Save the Best Model\nbest_model = models_to_test[best_model_name].to(device)\ntorch.save(best_model.state_dict(), f\"{best_model_name}_final.pth\")\nprint(f\"Best Model Saved as {best_model_name}_final.pth\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}