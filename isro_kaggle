{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport librosa\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-26T06:57:33.105855Z","iopub.execute_input":"2024-07-26T06:57:33.107161Z","iopub.status.idle":"2024-07-26T06:57:37.332924Z","shell.execute_reply.started":"2024-07-26T06:57:33.107114Z","shell.execute_reply":"2024-07-26T06:57:37.331304Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Constants\nDATA_PATH = \"/kaggle/input/ravdess-speech-song/\"\n\n# Define the model path within the Kaggle working directory\nMODEL_PATH = '/kaggle/working/model/voice_emotion_model.pth'","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:07:48.166781Z","iopub.execute_input":"2024-07-26T07:07:48.167250Z","iopub.status.idle":"2024-07-26T07:07:48.173683Z","shell.execute_reply.started":"2024-07-26T07:07:48.167215Z","shell.execute_reply":"2024-07-26T07:07:48.172234Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Create the model directory if it doesn't exist\nos.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:08:00.857098Z","iopub.execute_input":"2024-07-26T07:08:00.858263Z","iopub.status.idle":"2024-07-26T07:08:00.863927Z","shell.execute_reply.started":"2024-07-26T07:08:00.858223Z","shell.execute_reply":"2024-07-26T07:08:00.862550Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def extract_features(file_name):\n    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n    mfccs_processed = np.mean(mfccs.T, axis=0)\n    return mfccs_processed","metadata":{"execution":{"iopub.status.busy":"2024-07-26T06:57:37.343796Z","iopub.execute_input":"2024-07-26T06:57:37.344301Z","iopub.status.idle":"2024-07-26T06:57:37.356949Z","shell.execute_reply.started":"2024-07-26T06:57:37.344261Z","shell.execute_reply":"2024-07-26T06:57:37.355584Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def load_data(data_path):\n    features = []\n    labels = []\n    for actor_dir in os.listdir(data_path):\n        actor_path = os.path.join(data_path, actor_dir)\n        for file_name in os.listdir(actor_path):\n            file_path = os.path.join(actor_path, file_name)\n            mfccs = extract_features(file_path)\n            label = int(file_name.split('-')[2]) - 1  # Emotion labels are in the 3rd position\n            features.append(mfccs)\n            labels.append(label)\n    return np.array(features), np.array(labels)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T06:57:37.359694Z","iopub.execute_input":"2024-07-26T06:57:37.360086Z","iopub.status.idle":"2024-07-26T06:57:37.370670Z","shell.execute_reply.started":"2024-07-26T06:57:37.360027Z","shell.execute_reply":"2024-07-26T06:57:37.369332Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!pip install resampy\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T06:57:37.372201Z","iopub.execute_input":"2024-07-26T06:57:37.372554Z","iopub.status.idle":"2024-07-26T06:57:54.940885Z","shell.execute_reply.started":"2024-07-26T06:57:37.372521Z","shell.execute_reply":"2024-07-26T06:57:54.939296Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting resampy\n  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from resampy) (1.26.4)\nRequirement already satisfied: numba>=0.53 in /opt/conda/lib/python3.10/site-packages (from resampy) (0.58.1)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.53->resampy) (0.41.1)\nDownloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: resampy\nSuccessfully installed resampy-0.4.3\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load and preprocess data\nfeatures, labels = load_data(DATA_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T06:57:54.942986Z","iopub.execute_input":"2024-07-26T06:57:54.943462Z","iopub.status.idle":"2024-07-26T07:01:20.412575Z","shell.execute_reply.started":"2024-07-26T06:57:54.943422Z","shell.execute_reply":"2024-07-26T07:01:20.409107Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Encode labels\nlb = LabelBinarizer()\nlabels = lb.fit_transform(labels)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:01:20.414951Z","iopub.execute_input":"2024-07-26T07:01:20.416349Z","iopub.status.idle":"2024-07-26T07:01:20.436322Z","shell.execute_reply.started":"2024-07-26T07:01:20.416294Z","shell.execute_reply":"2024-07-26T07:01:20.434789Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:01:20.443937Z","iopub.execute_input":"2024-07-26T07:01:20.448556Z","iopub.status.idle":"2024-07-26T07:01:20.468257Z","shell.execute_reply.started":"2024-07-26T07:01:20.448471Z","shell.execute_reply":"2024-07-26T07:01:20.466495Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Convert to PyTorch tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_train = torch.tensor(np.argmax(y_train, axis=1), dtype=torch.long)\ny_test = torch.tensor(np.argmax(y_test, axis=1), dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:01:49.353290Z","iopub.execute_input":"2024-07-26T07:01:49.353731Z","iopub.status.idle":"2024-07-26T07:01:49.390852Z","shell.execute_reply.started":"2024-07-26T07:01:49.353698Z","shell.execute_reply":"2024-07-26T07:01:49.389661Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Define the model\nclass VoiceEmotionNN(nn.Module):\n    def __init__(self):\n        super(VoiceEmotionNN, self).__init__()\n        self.fc1 = nn.Linear(40, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 8)  # 8 emotion classes\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:01:55.450034Z","iopub.execute_input":"2024-07-26T07:01:55.451636Z","iopub.status.idle":"2024-07-26T07:01:55.461841Z","shell.execute_reply.started":"2024-07-26T07:01:55.451573Z","shell.execute_reply":"2024-07-26T07:01:55.460286Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model = VoiceEmotionNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:02:01.873198Z","iopub.execute_input":"2024-07-26T07:02:01.873710Z","iopub.status.idle":"2024-07-26T07:02:03.860335Z","shell.execute_reply.started":"2024-07-26T07:02:01.873669Z","shell.execute_reply":"2024-07-26T07:02:03.858895Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Training the model\nnum_epochs = 50  # Adjust number of epochs as needed\nfor epoch in range(num_epochs):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train)\n    loss = criterion(outputs, y_train)\n    loss.backward()\n    optimizer.step()\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:02:10.513830Z","iopub.execute_input":"2024-07-26T07:02:10.515153Z","iopub.status.idle":"2024-07-26T07:02:10.789875Z","shell.execute_reply.started":"2024-07-26T07:02:10.515102Z","shell.execute_reply":"2024-07-26T07:02:10.788353Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch [1/50], Loss: 15.3826\nEpoch [2/50], Loss: 8.2899\nEpoch [3/50], Loss: 6.4209\nEpoch [4/50], Loss: 4.8532\nEpoch [5/50], Loss: 4.3081\nEpoch [6/50], Loss: 3.9863\nEpoch [7/50], Loss: 3.8622\nEpoch [8/50], Loss: 3.9931\nEpoch [9/50], Loss: 3.9827\nEpoch [10/50], Loss: 3.8445\nEpoch [11/50], Loss: 3.8323\nEpoch [12/50], Loss: 4.0148\nEpoch [13/50], Loss: 3.7535\nEpoch [14/50], Loss: 3.2341\nEpoch [15/50], Loss: 2.9177\nEpoch [16/50], Loss: 2.7969\nEpoch [17/50], Loss: 2.6864\nEpoch [18/50], Loss: 2.6038\nEpoch [19/50], Loss: 2.6089\nEpoch [20/50], Loss: 2.5617\nEpoch [21/50], Loss: 2.4136\nEpoch [22/50], Loss: 2.2973\nEpoch [23/50], Loss: 2.2903\nEpoch [24/50], Loss: 2.3632\nEpoch [25/50], Loss: 2.3837\nEpoch [26/50], Loss: 2.2612\nEpoch [27/50], Loss: 2.1147\nEpoch [28/50], Loss: 2.0881\nEpoch [29/50], Loss: 2.1377\nEpoch [30/50], Loss: 2.1805\nEpoch [31/50], Loss: 2.1952\nEpoch [32/50], Loss: 2.1749\nEpoch [33/50], Loss: 2.1173\nEpoch [34/50], Loss: 2.0489\nEpoch [35/50], Loss: 2.0036\nEpoch [36/50], Loss: 1.9968\nEpoch [37/50], Loss: 2.0181\nEpoch [38/50], Loss: 2.0342\nEpoch [39/50], Loss: 2.0246\nEpoch [40/50], Loss: 2.0022\nEpoch [41/50], Loss: 1.9879\nEpoch [42/50], Loss: 1.9834\nEpoch [43/50], Loss: 1.9780\nEpoch [44/50], Loss: 1.9660\nEpoch [45/50], Loss: 1.9464\nEpoch [46/50], Loss: 1.9249\nEpoch [47/50], Loss: 1.9136\nEpoch [48/50], Loss: 1.9169\nEpoch [49/50], Loss: 1.9252\nEpoch [50/50], Loss: 1.9262\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the model\ntorch.save(model.state_dict(), MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:08:18.222195Z","iopub.execute_input":"2024-07-26T07:08:18.222702Z","iopub.status.idle":"2024-07-26T07:08:18.231058Z","shell.execute_reply.started":"2024-07-26T07:08:18.222666Z","shell.execute_reply":"2024-07-26T07:08:18.229747Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(X_test)\n    _, predicted = torch.max(outputs.data, 1)\n    accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n    print(f'Accuracy: {accuracy * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:08:32.051422Z","iopub.execute_input":"2024-07-26T07:08:32.051910Z","iopub.status.idle":"2024-07-26T07:08:32.071842Z","shell.execute_reply.started":"2024-07-26T07:08:32.051871Z","shell.execute_reply":"2024-07-26T07:08:32.070150Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Accuracy: 22.57%\n","output_type":"stream"}]},{"cell_type":"code","source":"# notebooks/train_model.ipynb\nimport os\nimport numpy as np\nimport librosa\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:10:56.372968Z","iopub.execute_input":"2024-07-26T07:10:56.373499Z","iopub.status.idle":"2024-07-26T07:10:56.381012Z","shell.execute_reply.started":"2024-07-26T07:10:56.373459Z","shell.execute_reply":"2024-07-26T07:10:56.379376Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def augment_audio(file_path):\n    audio, sample_rate = librosa.load(file_path, sr=None)\n    # Pitch shifting\n    pitch_shifted = librosa.effects.pitch_shift(audio, sr=sample_rate, n_steps=np.random.uniform(-2, 2))\n    # Time stretching\n    time_stretched = librosa.effects.time_stretch(audio, rate=np.random.uniform(0.8, 1.2))\n    # Adding noise\n    noise = np.random.randn(len(audio))\n    audio_noise = audio + 0.005 * noise\n    return [audio, pitch_shifted, time_stretched, audio_noise]","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:11:05.797515Z","iopub.execute_input":"2024-07-26T07:11:05.797975Z","iopub.status.idle":"2024-07-26T07:11:05.807776Z","shell.execute_reply.started":"2024-07-26T07:11:05.797939Z","shell.execute_reply":"2024-07-26T07:11:05.806092Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def extract_features_augmented(file_path):\n    audio_variants = augment_audio(file_path)\n    mfccs_list = []\n    for audio in audio_variants:\n        mfccs = librosa.feature.mfcc(y=audio, sr=16000, n_mfcc=40)\n        mfccs_processed = np.mean(mfccs.T, axis=0)\n        mfccs_list.append(mfccs_processed)\n    return mfccs_list","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:11:12.453426Z","iopub.execute_input":"2024-07-26T07:11:12.453848Z","iopub.status.idle":"2024-07-26T07:11:12.461355Z","shell.execute_reply.started":"2024-07-26T07:11:12.453818Z","shell.execute_reply":"2024-07-26T07:11:12.459902Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def load_data(data_path):\n    features = []\n    labels = []\n    for actor_dir in os.listdir(data_path):\n        actor_path = os.path.join(data_path, actor_dir)\n        for file_name in os.listdir(actor_path):\n            file_path = os.path.join(actor_path, file_name)\n            mfccs_list = extract_features_augmented(file_path)\n            label = int(file_name.split('-')[2]) - 1\n            for mfccs in mfccs_list:\n                features.append(mfccs)\n                labels.append(label)\n    return np.array(features), np.array(labels)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:11:18.989619Z","iopub.execute_input":"2024-07-26T07:11:18.990023Z","iopub.status.idle":"2024-07-26T07:11:18.998288Z","shell.execute_reply.started":"2024-07-26T07:11:18.989993Z","shell.execute_reply":"2024-07-26T07:11:18.996958Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"features, labels = load_data(DATA_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T07:11:25.470455Z","iopub.execute_input":"2024-07-26T07:11:25.470922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nlb = LabelBinarizer()\nlabels = lb.fit_transform(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = torch.tensor(X_train, dtype=torch.float32)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_train = torch.tensor(np.argmax(y_train, axis=1), dtype=torch.long)\ny_test = torch.tensor(np.argmax(y_test, axis=1), dtype=torch.long)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImprovedVoiceEmotionNN(nn.Module):\n    def __init__(self):\n        super(ImprovedVoiceEmotionNN, self).__init__()\n        self.fc1 = nn.Linear(40, 256)\n        self.dropout1 = nn.Dropout(0.3)\n        self.fc2 = nn.Linear(256, 128)\n        self.dropout2 = nn.Dropout(0.3)\n        self.fc3 = nn.Linear(128, 64)\n        self.dropout3 = nn.Dropout(0.3)\n        self.fc4 = nn.Linear(64, 8)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.relu(self.fc3(x))\n        x = self.dropout3(x)\n        x = self.fc4(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}